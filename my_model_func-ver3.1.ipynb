{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit1(dtrain, dtest, predictors,useTrainCV=True, thresh = .8, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    alg = XGBClassifier(\n",
    "        learning_rate =lr,\n",
    "        n_estimators=ne,\n",
    "        max_depth=md,\n",
    "        min_child_weight=mcw,\n",
    "        gamma=g,\n",
    "        subsample=ss,\n",
    "        colsample_bytree=cst,\n",
    "        objective= obj,\n",
    "        nthread=nt,\n",
    "        scale_pos_weight=spw,\n",
    "        seed=seed)\n",
    "    \n",
    "    print (\"Parameter Tuning\")\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        print (\"Current Paramter:\\n\", xgb_param)\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        xgtest = xgb.DMatrix(dtest[predictors].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics = 'auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
    "        print (cvresult)\n",
    "        ne_new = n_estimators=cvresult.shape[0]\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print (\"\\nNew paramter\\n\", alg.get_xgb_params())\n",
    "        \n",
    "    #Fit the algorithm on the data\n",
    "    print (\"Fitting Algorithm\")\n",
    "    eval_set = [(dtrain[predictors],dtrain[target]), (dtest[predictors],dtest[target])]\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc', eval_set = eval_set, \n",
    "            verbose = True, early_stopping_rounds=early_stopping_rounds) #scoring_xg\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    dtrain_predictions = (dtrain_predprob > thresh).astype(int)\n",
    "            \n",
    "    #Print training data report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % (metrics.accuracy_score(dtrain[target].values, dtrain_predictions)*100))\n",
    "    print (\"AUC Score (Train): %f\" % (metrics.roc_auc_score(dtrain[target], dtrain_predprob)))\n",
    "    print (\"F1 score (Train): %f\" % (metrics.f1_score(dtrain[target], dtrain_predictions)*100))\n",
    "    print (\"Precision score (Train): %f\" % (metrics.precision_score(dtrain[target], dtrain_predictions)*100))\n",
    "    print (\"Recall score (Train): %f\" % (metrics.recall_score(dtrain[target], dtrain_predictions)*100))\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    results = dtest.copy()\n",
    "    results['predprob'] = alg.predict_proba(results[predictors])[:,1]\n",
    "    results['predict'] = (results['predprob'] > thresh).astype(int)\n",
    "    \n",
    "    #Print validation data report:\n",
    "    print ('\\n AUC Score (Valid): %f' % (metrics.roc_auc_score(results[target], results['predprob'])))\n",
    "    print (\"F1 score (Valid): %f\" % (metrics.f1_score(results[target], results['predict'])*100))\n",
    "    print (\"Precision score (Valid): %f\" % (metrics.precision_score(results[target], results['predict'])*100))\n",
    "    print (\"Recall score (Valid): %f\" % (metrics.recall_score(results[target], results['predict'])*100))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances', figsize = (20, 5))\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    # retrieve performance metrics\n",
    "    results = alg.evals_result()\n",
    "    epochs = len(results['validation_0']['auc'])\n",
    "    x_axis = range(0, epochs)\n",
    "    \n",
    "    # plot auc\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('auc')\n",
    "#     plt.title()\n",
    "    return alg#, ne_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using my_model_func-ver2+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('mode.chained_assignment', 'raise')\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "import itertools as itool\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import  metrics\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import datetime\n",
    "from io import StringIO\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('using my_model_func-ver3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Market List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_market_list(status = 'active'):\n",
    "    headers = {\n",
    "   'Origin': 'https://app.prediction.exchange/',\n",
    "   'Accept-Encoding': 'gzip, deflate',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36',\n",
    "   'Content-Type': 'application/json;charset=UTF-8',\n",
    "   'Accept': 'application/json, text/plain, */*',\n",
    "   'Referer': 'https://app.prediction.exchange/csv-download',\n",
    "   'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    r = requests.get('https://app.prediction.exchange/api/btrx/market', headers=headers)\n",
    "    active_curr_list = json.loads(r.text)['response']\n",
    "    \n",
    "    r = requests.get('https://app.prediction.exchange/api/btrx/market/inactive', headers=headers)\n",
    "    all_curr_list = json.loads(r.text)['response']\n",
    "    all_curr_list = [d['name'] for d in all_curr_list]\n",
    "    \n",
    "    inactive_curr_list = [c for c in all_curr_list if c not in active_curr_list]\n",
    "    \n",
    "    if status == 'active':\n",
    "        return active_curr_list\n",
    "    elif status == 'all':\n",
    "        return all_curr_list\n",
    "    elif status == 'inactive':\n",
    "        return inactive_curr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_csv_2(start_date, end_date, data_type):\n",
    "   \n",
    "    date_range = pd.date_range(start=start_date, end=data_date, freq='D')\n",
    "    folder_name_list = list(set([(100*(x.year%2000) + x.month) for x in date_range]))\n",
    "\n",
    "    if data_type == 'trade':\n",
    "        data_path = '/home/ambuj/rolled-up_data_new'\n",
    "    elif data_type == 'order':\n",
    "        data_path = '/home/ambuj/order_data_new'\n",
    "\n",
    "    final_data_file_created = 0\n",
    "    final_data = pd.DataFrame()\n",
    "    for fold_ind, folder in enumerate(folder_name_list):\n",
    "        final_data_path = os.path.join(data_path, str(folder))\n",
    "        trade_file_name = os.path.join(final_data_path, list_d[list_i]+'.csv')\n",
    "\n",
    "        if os.path.isfile(trade_file_name):\n",
    "            data = fetch_data_csv(final_data_path, list_d[list_i]+'.csv', data_type)\n",
    "            if data.empty: ## Confirm if this is to be removed\n",
    "    #                 print (\"Data Empty\", folder)\n",
    "               continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if final_data_file_created == 0:\n",
    "            final_data = pd.DataFrame(columns = data.columns)\n",
    "            for col in final_data.columns:\n",
    "                final_data[col] = final_data[col].astype(data[col].dtype)\n",
    "            final_data_file_created = 1\n",
    "\n",
    "        final_data = pd.concat([final_data, data])\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laod the all data at once in dictionary\n",
    "\n",
    "def load_data(data_path, curr, folder_name_list, data_type):\n",
    "   \n",
    "    print(curr,data_type)\n",
    "    \n",
    "    final_data_created = 0\n",
    "    final_data = pd.DataFrame()\n",
    "    for fold_ind, folder in enumerate(folder_name_list):\n",
    "        \n",
    "#         print(folder)\n",
    "        \n",
    "        final_data_path = os.path.join(data_path, str(folder))\n",
    "        \n",
    "#         print(final_data_path)\n",
    "        \n",
    "        data_file_name = os.path.join(final_data_path, curr+'.csv')\n",
    "\n",
    "        if os.path.isfile(data_file_name):\n",
    "            data = fetch_data_csv(final_data_path, curr+'.csv', data_type) \n",
    "#             print(data.shape)\n",
    "\n",
    "            if data.empty: ## Confirm if this is to be removed\n",
    "                print (\"Data Empty\", folder)\n",
    "                continue\n",
    "        else:\n",
    "            print (\"File does not exist!\", folder)\n",
    "            continue\n",
    "\n",
    "        if final_data_created == 0:\n",
    "            final_data = pd.DataFrame(columns = data.columns)\n",
    "            for col in final_data.columns:\n",
    "                final_data[col] = final_data[col].astype(data[col].dtype)\n",
    "            final_data_created = 1\n",
    "\n",
    "        final_data = pd.concat([final_data, data])\n",
    "#         print(final_data.shape)\n",
    "   \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_csv(path, market, data_type):\n",
    "    \n",
    "    if data_type == 'trade':\n",
    "        try:\n",
    "            data = pd.read_csv(os.path.join(path,market), index_col=['Timestamp'], parse_dates=['Timestamp'])\n",
    "        except:\n",
    "            data = pd.DataFrame()\n",
    "    elif data_type == 'order':\n",
    "        try:\n",
    "            data = pd.read_csv(os.path.join(path,market), index_col=['timestamp'], parse_dates=['timestamp'], low_memory = False)\n",
    "        except:\n",
    "            data = pd.DataFrame()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NA values\n",
    "\n",
    "def fill_na(df, vol_win = 48*60):\n",
    "    \n",
    "    # To get the data at unrecorded timestamps\n",
    "    df_temp = df.resample('T').mean()\n",
    "    \n",
    "    # to fill missing Currency Pair values\n",
    "    if not df_temp.empty:\n",
    "        df_temp.loc[:, 'Curreny_Pair'] = df.at[df.index[0],'Curreny_Pair']\n",
    "#         df_temp.loc[:, '_id'] = df.at[df.index[0],'_id']\n",
    "    \n",
    "    # To fill volumes and no. of transaction by last 48 hrs average\n",
    "#     for col in ['Volume_first_currency','Volume_second_currency' ,'No_of_transaction' ]:\n",
    "#         df_temp[col] = df[col].fillna(pd.rolling_mean(df[col], vol_win, min_periods=1))\n",
    "    df_temp['Volume_first_currency'].fillna(df_temp['Volume_first_currency'].rolling(window = vol_win, min_periods = 1).mean(), inplace = True)\n",
    "    df_temp['Volume_second_currency'].fillna(df_temp['Volume_second_currency'].rolling(window = vol_win, min_periods = 1).mean(), inplace = True)\n",
    "    df_temp['No_of_transaction'].fillna(df_temp['No_of_transaction'].rolling(window = vol_win, min_periods = 1).mean(), inplace = True)\n",
    "\n",
    "    # To fill Prices by forward values\n",
    "#     for col in ['Open', 'High', 'Low', 'Close', 'Weighted_price']:\n",
    "    df_temp['Open'].fillna(method='ffill', inplace=True)\n",
    "    df_temp['High'].fillna(method='ffill', inplace=True)\n",
    "    df_temp['Low'].fillna(method='ffill', inplace=True)\n",
    "    df_temp['Close'].fillna(method='ffill', inplace=True)\n",
    "    df_temp['Weighted_price'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "#     # To drop initial rows with na values\n",
    "#     df_temp.dropna(subset=['Open'],inplace=True)\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSI\n",
    "\n",
    "def gain(x):\n",
    "    if(pd.isnull(x)):\n",
    "        return np.nan\n",
    "    elif(x>0):\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def loss(x):\n",
    "    if(pd.isnull(x)):\n",
    "        return np.nan\n",
    "    elif(x<0):\n",
    "        return abs(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def rsi(df, rsi_win_array, rolling_win):\n",
    "    \n",
    "    df_original = df.copy()\n",
    "    df_temp = df.copy()\n",
    "    d={}\n",
    "    interval = 15\n",
    "\n",
    "    df_temp['change'] = df_temp['Close'] - df_temp['Close'].shift(rolling_win)\n",
    "    df_temp['gain'] = df_temp['change'].apply(lambda x: gain(x))\n",
    "    df_temp['loss'] = df_temp['change'].apply(lambda x: loss(x))\n",
    "\n",
    "        \n",
    "    for rsi_win in rsi_win_array:\n",
    "        \n",
    "        temp = df_temp.copy()\n",
    "        \n",
    "        for i in range(int(rolling_win/interval)):\n",
    "\n",
    "            temp['rs'] = temp.loc[::rolling_win, 'gain'].rolling(rsi_win).mean()/ \\\n",
    "                         temp.loc[::rolling_win,'loss'].rolling(rsi_win).mean()\n",
    "        \n",
    "            df_temp_1 = temp.dropna(subset=['rs'],axis=0, how='any')\n",
    "            d[\"{}\".format(i)] = df_temp_1\n",
    "            temp.drop(temp.index[:interval], inplace = True)\n",
    "    \n",
    "\n",
    "        result =  pd.concat([ d['{}'.format(j)] for j in range(len(d)) ])\n",
    "        result.sort_index(inplace=True)\n",
    "\n",
    "        df_original['rs'] = result['rs']\n",
    "        df_original['rsi'+str(rsi_win)] = 100 - (100/(1+df_original['rs']))\n",
    "        df_original.drop('rs', axis = 1, inplace = True)\n",
    "\n",
    "    return df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume Average\n",
    "\n",
    "def volume_feat(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['volume_acc_14_24_60'] = temp['Volume_first_currency'].rolling(14*24*60, min_periods = 12*24*60).sum()\n",
    "    temp['volume_accu_168_60'] = temp['Volume_first_currency'].rolling(168*60, min_periods = 144*60).sum()\n",
    "    temp['volume_accu_144_60'] = temp['Volume_first_currency'].rolling(144*60, min_periods = 120*60).sum()\n",
    "    temp['volume_accu_120_60'] = temp['Volume_first_currency'].rolling(120*60, min_periods = 96*60).sum()\n",
    "    temp['volume_accu_96_60'] = temp['Volume_first_currency'].rolling(96*60, min_periods = 72*60).sum()\n",
    "    temp['volume_accu_72_60'] = temp['Volume_first_currency'].rolling(72*60, min_periods = 54*60).sum()\n",
    "    temp['volume_accu_48_60'] = temp['Volume_first_currency'].rolling(48*60, min_periods = 36*60).sum()\n",
    "    temp['volume_accu_24_60'] = temp['Volume_first_currency'].rolling(24*60, min_periods = 18*60).sum()\n",
    "    temp['volume_accu_240'] = temp['Volume_first_currency'].rolling(240, min_periods = 200).sum()\n",
    "    temp['volume_accu_120'] = temp['Volume_first_currency'].rolling(120, min_periods = 100).sum()\n",
    "    temp['volume_accu_60'] = temp['Volume_first_currency'].rolling(60, min_periods = 50).sum()\n",
    "    temp['volume_accu_30'] = temp['Volume_first_currency'].rolling(30, min_periods = 25).sum()\n",
    "    temp['volume_accu_15'] = temp['Volume_first_currency'].rolling(15, min_periods = 13).sum()\n",
    "    temp['volume_accu_10'] = temp['Volume_first_currency'].rolling(10, min_periods = 10).sum()\n",
    "    temp.iloc[:, -13:] = (temp.iloc[:,-13:].div(temp.volume_acc_14_24_60, axis =0)).astype(float)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Average\n",
    "\n",
    "def price_mean_feat(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['price_mean_14_24_60'] = temp['Weighted_price'].rolling(14*24*60, min_periods = 12*24*60).sum() #----> correct this\n",
    "    temp['price_mean_168_60'] = temp['Weighted_price'].rolling(168*60, min_periods = 144*60).mean()\n",
    "    temp['price_mean_144_60'] = temp['Weighted_price'].rolling(144*60, min_periods = 120*60).mean()\n",
    "    temp['price_mean_120_60'] = temp['Weighted_price'].rolling(120*60, min_periods = 96*60).mean()\n",
    "    temp['price_mean_96_60'] = temp['Weighted_price'].rolling(96*60, min_periods = 72*60).mean()\n",
    "    temp['price_mean_72_60'] = temp['Weighted_price'].rolling(72*60, min_periods = 54*60).mean()\n",
    "    temp['price_mean_48_60'] = temp['Weighted_price'].rolling(48*60, min_periods = 36*60).mean()\n",
    "    temp['price_mean_24_60'] = temp['Weighted_price'].rolling(24*60, min_periods = 18*60).mean()\n",
    "    temp['price_mean_240'] = temp['Weighted_price'].rolling(240, min_periods = 200).mean()\n",
    "    temp['price_mean_120'] = temp['Weighted_price'].rolling(120, min_periods = 100).mean()\n",
    "    temp['price_mean_60'] = temp['Weighted_price'].rolling(60, min_periods = 50).mean()\n",
    "    temp['price_mean_30'] = temp['Weighted_price'].rolling(30, min_periods = 25).mean()\n",
    "    temp['price_mean_15'] = temp['Weighted_price'].rolling(15, min_periods = 13).mean()\n",
    "    temp['price_mean_10'] = temp['Weighted_price'].rolling(10, min_periods = 10).mean()\n",
    "    temp['current_price'] = temp['Close']\n",
    "    temp.iloc[:, -14:] = (temp.iloc[:,-14:].div(temp.price_mean_14_24_60, axis =0)).astype(float)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# William\n",
    "\n",
    "def william(data, rolling_win = 14*24*60):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    highest_high = temp['High'].rolling(rolling_win).max()\n",
    "    lowest_low = temp['Low'].rolling(rolling_win).min()\n",
    "    temp['william'] = ( (highest_high - temp['Close']) / (highest_high - lowest_low) * -100 )\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoch_osci\n",
    "\n",
    "def stoch_osci(data, rolling_win = 14*24*60):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    highest_high = temp['High'].rolling(rolling_win).max()\n",
    "    lowest_low = temp['Low'].rolling(rolling_win).min()\n",
    "    temp['stoch_osci'] = ( (temp['Close']-lowest_low) / (highest_high - lowest_low) ) #* 100 ) ---> correct this\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_book_feature_power(data, trade_feat):\n",
    "    \n",
    "    #Convert dtypes to float\n",
    "    print(data.columns)\n",
    "    data.iloc[:,2:] = data.iloc[:,2:].astype(np.float64)\n",
    "#     print (data['powerImbalancePower4Buy5Percent'].dtype)\n",
    "#     k = data['powerImbalancePower4Buy5Percent'].apply(pd.to_numeric, errors='coerce')\n",
    "#     print (k.dtype)\n",
    "    #Impute missing data\n",
    "    data.update(data[list(data.columns[14:])].fillna(method='ffill'))\n",
    "    \n",
    "    #Power Imbalance\n",
    "    data_feat = pd.DataFrame(index = data.index)\n",
    "    \n",
    "    data_feat['pow_imbal_2_5'] = data['powerImbalancePower2Buy5Percent'] - data['powerImbalancePower2Sell5Percent']\n",
    "    data_feat['pow_imbal_2_10'] = data['powerImbalancePower2Buy10Percent'] - data['powerImbalancePower2Sell10Percent']\n",
    "    data_feat['pow_imbal_2_15'] = data['powerImbalancePower2Buy15Percent'] - data['powerImbalancePower2Sell15Percent']\n",
    "    data_feat['pow_imbal_2_20'] = data['powerImbalancePower2Buy20Percent'] - data['powerImbalancePower2Sell20Percent']\n",
    "    data_feat['pow_imbal_2_25'] = data['powerImbalancePower2Buy25Percent'] - data['powerImbalancePower2Sell25Percent']\n",
    "    data_feat['pow_imbal_2_all'] = data['powerImbalancePower2BuyAll'] - data['powerImbalancePower2SellAll']\n",
    "    \n",
    "    data_feat['pow_imbal_4_5'] = data['powerImbalancePower4Buy5Percent'] - data['powerImbalancePower4Sell5Percent']\n",
    "    data_feat['pow_imbal_4_10'] = data['powerImbalancePower4Buy10Percent'] - data['powerImbalancePower4Sell10Percent']\n",
    "    data_feat['pow_imbal_4_15'] = data['powerImbalancePower4Buy15Percent'] - data['powerImbalancePower4Sell15Percent']\n",
    "    data_feat['pow_imbal_4_20'] = data['powerImbalancePower4Buy20Percent'] - data['powerImbalancePower4Sell20Percent']\n",
    "    data_feat['pow_imbal_4_25'] = data['powerImbalancePower4Buy25Percent'] - data['powerImbalancePower4Sell25Percent']\n",
    "    data_feat['pow_imbal_4_all'] = data['powerImbalancePower4BuyAll'] - data['powerImbalancePower4SellAll']\n",
    "    \n",
    "    data_feat['pow_imbal_8_5'] = data['powerImbalancePower8Buy5Percent'] - data['powerImbalancePower8Sell5Percent']\n",
    "    data_feat['pow_imbal_8_10'] = data['powerImbalancePower8Buy10Percent'] - data['powerImbalancePower8Sell10Percent']\n",
    "    data_feat['pow_imbal_8_15'] = data['powerImbalancePower8Buy15Percent'] - data['powerImbalancePower8Sell15Percent']\n",
    "    data_feat['pow_imbal_8_20'] = data['powerImbalancePower8Buy20Percent'] - data['powerImbalancePower8Sell20Percent']\n",
    "    data_feat['pow_imbal_8_25'] = data['powerImbalancePower8Buy25Percent'] - data['powerImbalancePower8Sell25Percent']\n",
    "    data_feat['pow_imbal_8_all'] = data['powerImbalancePower8BuyAll'] - data['powerImbalancePower8SellAll']\n",
    "    \n",
    "    data_feat['pow_adj_2_5'] = data['powerAdjustedPower2Buy5Percent'] - data['powerAdjustedPower2Sell5Percent']\n",
    "    data_feat['pow_adj_2_10'] = data['powerAdjustedPower2Buy10Percent'] - data['powerAdjustedPower2Sell10Percent']\n",
    "    data_feat['pow_adj_2_15'] = data['powerAdjustedPower2Buy15Percent'] - data['powerAdjustedPower2Sell15Percent']\n",
    "    data_feat['pow_adj_2_20'] = data['powerAdjustedPower2Buy20Percent'] - data['powerAdjustedPower2Sell20Percent']\n",
    "    data_feat['pow_adj_2_25'] = data['powerAdjustedPower2Buy25Percent'] - data['powerAdjustedPower2Sell25Percent']\n",
    "    data_feat['pow_adj_2_all'] = data['powerAdjustedPower2BuyAll'] - data['powerAdjustedPower2SellAll']\n",
    "    \n",
    "    data_feat['pow_adj_4_5'] = data['powerAdjustedPower4Buy5Percent'] - data['powerAdjustedPower4Sell5Percent']\n",
    "    data_feat['pow_adj_4_10'] = data['powerAdjustedPower4Buy10Percent'] - data['powerAdjustedPower4Sell10Percent']\n",
    "    data_feat['pow_adj_4_15'] = data['powerAdjustedPower4Buy15Percent'] - data['powerAdjustedPower4Sell15Percent']\n",
    "    data_feat['pow_adj_4_20'] = data['powerAdjustedPower4Buy20Percent'] - data['powerAdjustedPower4Sell20Percent']\n",
    "    data_feat['pow_adj_4_25'] = data['powerAdjustedPower4Buy25Percent'] - data['powerAdjustedPower4Sell25Percent']\n",
    "    data_feat['pow_adj_4_all'] = data['powerAdjustedPower4BuyAll'] - data['powerAdjustedPower4SellAll']\n",
    "    \n",
    "    data_feat['pow_adj_8_5'] = data['powerAdjustedPower8Buy5Percent'] - data['powerAdjustedPower8Sell5Percent']\n",
    "    data_feat['pow_adj_8_10'] = data['powerAdjustedPower8Buy10Percent'] - data['powerAdjustedPower8Sell10Percent']\n",
    "    data_feat['pow_adj_8_15'] = data['powerAdjustedPower8Buy15Percent'] - data['powerAdjustedPower8Sell15Percent']\n",
    "    data_feat['pow_adj_8_20'] = data['powerAdjustedPower8Buy20Percent'] - data['powerAdjustedPower8Sell20Percent']\n",
    "    data_feat['pow_adj_8_25'] = data['powerAdjustedPower8Buy25Percent'] - data['powerAdjustedPower8Sell25Percent']\n",
    "    data_feat['pow_adj_8_all'] = data['powerAdjustedPower8BuyAll'] - data['powerAdjustedPower8SellAll']\n",
    "    \n",
    "    out = pd.concat([data_feat, trade_feat], axis = 1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_book_feature_volume(data, trade_feat):\n",
    "    \n",
    "    #Convert dtypes to float\n",
    "    data.iloc[:,2:] = data.iloc[:,2:].apply(pd.to_numeric, errors = 'coerce')\n",
    "    \n",
    "    \n",
    "    #Impute missing data\n",
    "    data.update(data[list(data.columns[14:])].fillna(method='ffill'))\n",
    "    \n",
    "    #Power Imbalance\n",
    "    data_feat = pd.DataFrame(index = data.index)\n",
    "    data_feat['volume_acc_14_24_60'] = trade_feat['Volume_second_currency'].rolling(14*24*60, min_periods = 13*24*60).sum()\n",
    "    data_feat['order_vol_buy_5'] = data['orderVolumeBuySecondaryCurrency5Percent']\n",
    "    data_feat['order_vol_buy_10'] = data['orderVolumeBuySecondaryCurrency10Percent']\n",
    "    data_feat['order_vol_buy_15'] = data['orderVolumeBuySecondaryCurrency15Percent']\n",
    "    data_feat['order_vol_buy_20'] = data['orderVolumeBuySecondaryCurrency20Percent']\n",
    "    data_feat['order_vol_buy_25'] = data['orderVolumeBuySecondaryCurrency25Percent']\n",
    "    data_feat['order_vol_buy_All'] = data['orderVolumeBuySecondaryCurrencyAll']\n",
    "    \n",
    "    data_feat['order_vol_sell_5'] = data['orderVolumeSellSecondaryCurrency5Percent']\n",
    "    data_feat['order_vol_sell_10'] = data['orderVolumeSellSecondaryCurrency10Percent']\n",
    "    data_feat['order_vol_sell_15'] = data['orderVolumeSellSecondaryCurrency15Percent']\n",
    "    data_feat['order_vol_sell_20'] = data['orderVolumeSellSecondaryCurrency20Percent']\n",
    "    data_feat['order_vol_sell_25'] = data['orderVolumeSellSecondaryCurrency25Percent']\n",
    "    data_feat['order_vol_sell_All'] = data['orderVolumeSellSecondaryCurrencyAll']\n",
    "    \n",
    "    data_feat['order_vol_diff_5'] = data_feat['order_vol_buy_5'] - data_feat['order_vol_sell_5']\n",
    "    data_feat['order_vol_diff_10'] = data_feat['order_vol_buy_10'] - data_feat['order_vol_sell_10']\n",
    "    data_feat['order_vol_diff_15'] = data_feat['order_vol_buy_15'] - data_feat['order_vol_sell_15']\n",
    "    data_feat['order_vol_diff_20'] = data_feat['order_vol_buy_20'] - data_feat['order_vol_sell_20']\n",
    "    data_feat['order_vol_diff_25'] = data_feat['order_vol_buy_25'] - data_feat['order_vol_sell_25']\n",
    "    data_feat['order_vol_diff_All'] = data_feat['order_vol_buy_All'] - data_feat['order_vol_sell_All']\n",
    "    \n",
    "    data_feat = data_feat.iloc[:, 1:].div(data_feat.iloc[:,0], axis = 0)\n",
    "    \n",
    "    out = pd.concat([trade_feat, data_feat], axis = 1)\n",
    "#     out.dropna(inplace=True)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate Jump value(target variable)\n",
    "\n",
    "def calc_jump(df, jump, window):\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    win_min = window*60\n",
    "    \n",
    "    df_temp['highest_high'] = df_temp.High[::-1].rolling(win_min).max()[::-1] #highest high in x hr taken\n",
    "\n",
    "    df_temp['pct_change_{}_{}'.format(jump,window)] = (( df_temp['highest_high'] - df_temp['Open']) / df_temp['Open'] ) * 100\n",
    "    df_temp['class_{}_{}'.format(jump,window)] = np.where(df_temp['pct_change_{}_{}'.format(jump,window)] >= jump, 1, 0) \n",
    "    \n",
    "    # # To drop all the NA Values\n",
    "#     df_temp.dropna(subset=['highest_high'],inplace=True)\n",
    "    \n",
    "    #     To drop highest high column\n",
    "    df_temp.drop(['highest_high'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pct(df,train_size,validation_size):\n",
    "     \n",
    "    delta =  (df.index[-1] - df.index[0]) \n",
    "    \n",
    "    train_end = (df.index[0] + (train_size*delta))\n",
    "    \n",
    "    train = df[df.index < train_end]\n",
    "    temp_df = df[df.index >= train_end]\n",
    "    \n",
    "    valid_end = (temp_df.index[0] + validation_size*delta)\n",
    "    \n",
    "    validation = df[(df.index > train_end) & (df.index < valid_end)]\n",
    "    \n",
    "    test = df[df.index >= valid_end]\n",
    "    \n",
    "    return train,validation,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_analysis_2(model, input_d, actual, thresh_start, hours):\n",
    "\n",
    "## Exdtension of thresh_analysis where we consider only 1 prediction in x hours for each currency\n",
    "    input_d['class_{}_{}'.format(jump,window)] = input_d.loc[:,'class_{}_{}'.format(jump,window)].astype(int)\n",
    "    input_d = input_d.iloc[::sample_int_test,:]\n",
    "    \n",
    "    col = ['thresh','precision', 'recall', 'predict_count', 'precision_sel', 'predict_count_sel', 'Unique_Currency']\n",
    "    thresh_res = pd.DataFrame(columns = col)\n",
    "    ind = 0\n",
    "    \n",
    "    for t in np.arange(thresh_start, 100, 1):\n",
    "        t /= 100.0\n",
    "        print (\"Threshold: \", t)\n",
    "        preds = model.predict_proba(input_d[feature])\n",
    "        input_d['pred'] = (preds[:,1] >= t).astype(int)\n",
    "#         print(len(input_d['pred']))\n",
    "        #Calculating precison coonsidering prediction at the interval of 24 hours\n",
    "        temp = input_d[input_d.pred == 1]\n",
    "#         print(len(temp['pred']))\n",
    "        curr_unique = temp['Curreny_Pair'].unique()\n",
    "#         print(curr_unique)\n",
    "        \n",
    "        new_input_d = pd.DataFrame()\n",
    "        for curr in curr_unique:\n",
    "#             print (\"Currency Pair: \", curr)\n",
    "            temp1 = temp[temp.Curreny_Pair == curr][['Curreny_Pair','pred','class_{}_{}'.format(jump,window)]]\n",
    "#             print(temp1)\n",
    "            \n",
    "            while len(temp1) != 0:\n",
    "                curr_time = temp1.index[0]\n",
    "                new_input_d = pd.concat([new_input_d, temp1[temp1.index == curr_time]])\n",
    "                drop_time = curr_time + timedelta(hours = hours)\n",
    "                temp1 = temp1[temp1.index >= drop_time]\n",
    "        \n",
    "        if len(new_input_d) == 0:\n",
    "            break\n",
    "        \n",
    "        thresh_res.loc[ind, col] = int(t*100), metrics.precision_score(input_d[actual], input_d['pred'])*100, \\\n",
    "                                metrics.recall_score(input_d[actual], input_d['pred'])*100, sum(input_d['pred']), \\\n",
    "                                (new_input_d['class_{}_{}'.format(jump,window)].sum()/float(len(new_input_d)))*100, len(new_input_d), len(curr_unique)\n",
    "        ind = ind + 1\n",
    "        \n",
    "    ## Calculating best threshold and corresposnding precison\n",
    "    prev_prec = 0\n",
    "    cut_off_thresh = 0\n",
    "    for ind, row in thresh_res.iterrows():\n",
    "        if row['precision'] > prev_prec:\n",
    "            cut_off_thresh = row['thresh']\n",
    "            prev_prec = row['precision']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print (\"Best Threshold and corresponding select precision: \", cut_off_thresh, thresh_res.at[ind-1, 'precision_sel'])\n",
    "    return thresh_res, cut_off_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_precision(input_d, y, hours):\n",
    "    \n",
    "    res_col = ['predictions'] + y\n",
    "    \n",
    "    temp = input_d[input_d.predictions == 1]\n",
    "    daily_results_raw = temp.resample('D').sum()\n",
    "    for col in y:\n",
    "        daily_results_raw['precision_raw_{}'.format(col)] = (daily_results_raw[col]/daily_results_raw.predictions)*100\n",
    "        \n",
    "    daily_results_raw.rename(columns = {'predictions': 'predictions_raw'}, inplace = True)\n",
    "    \n",
    "    curr_unique = temp['Curreny_Pair'].unique()\n",
    "    new_input_d = pd.DataFrame()\n",
    "    for curr in curr_unique:\n",
    "#             print (\"Currency Pair: \", curr)\n",
    "        temp1 = temp[temp.Curreny_Pair == curr]#[['Currency_Pair','predictions','class_20']]\n",
    "    \n",
    "        while len(temp1) != 0:\n",
    "            curr_time = temp1.index[0]\n",
    "            new_input_d = pd.concat([new_input_d, temp1[temp1.index == curr_time]])\n",
    "            drop_time = curr_time + timedelta(hours = hours)\n",
    "            temp1 = temp1[temp1.index >= drop_time]\n",
    "    \n",
    "    \n",
    "    daily_res = new_input_d.resample('D').sum()\n",
    "    \n",
    "    for col in y:\n",
    "        daily_res['precision_sel_{}'.format(col)] = (daily_res[col]/daily_res.predictions)*100\n",
    "    \n",
    "    \n",
    "    daily_res.rename(columns = {'predictions': 'predictions_sel'}, inplace = True)\n",
    "    \n",
    "    output = pd.merge(daily_results_raw, daily_res, left_index = True, right_index = True)\n",
    " \n",
    "    output_summary = pd.DataFrame(columns = ['class_type', 'raw', 'sel']) ## Summary output dataframe\n",
    "    \n",
    "    for out_ind, col in enumerate(y):\n",
    "        overall_raw_precision = (float(output[col+'_x'].sum())/float(output['predictions_raw'].sum()))*100\n",
    "        overall_sel_precision = (float(output[col+'_y'].sum())/float(output['predictions_sel'].sum()))*100\n",
    "        output_summary.at[out_ind, ['class_type','raw', 'sel']] = 'precision_'+col, overall_raw_precision, overall_sel_precision\n",
    "    output_summary.at[out_ind+1, ['class_type','raw', 'sel']] = 'Total_predictions_count' \\\n",
    "                                , output['predictions_raw'].sum(), output['predictions_sel'].sum()\n",
    "    \n",
    "    output_col_raw, output_col_sel = [],[]\n",
    "    \n",
    "    for col in y:\n",
    "        output_col_raw.append('precision_raw_'+col)\n",
    "        output_col_sel.append('precision_sel_'+col)\n",
    "    \n",
    "    output_col_raw.append('predictions_raw')\n",
    "    output_col_sel.append('predictions_sel')\n",
    "    \n",
    "    output_raw = output[output_col_raw]\n",
    "    output_sel = output[output_col_sel]\n",
    "    \n",
    "    return output_summary, output_raw, output_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot_importance(booster, figsize, **kwargs): \n",
    "    from matplotlib import pyplot as plt\n",
    "    from xgboost import plot_importance\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
